{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Nanodegree Capstone Project\n",
    "This notebook implements ETL process to extract bike sharing data from CSV files located in a public AWS S3 bucket.\n",
    "\n",
    "Data then is transformed into dimentional model using Spark running on an AWS EMR cluster.\n",
    "\n",
    "Finally, dimentional tables are loaded to the same S3 bucket to allow for infinite scalling and to save cost on spark cluster.\n",
    "\n",
    "For more details about the goal and scope of the project read the `README.md` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Importing PySpark sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "loaing the CSV files from S3 bucket `omar-dend`. It is located in `us-west-2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_station_df = spark.read.csv('s3://omar-dend/station.csv', header=True)\n",
    "st_weather_df = spark.read.csv('s3://omar-dend/weather.csv', header=True)\n",
    "st_trip_df = spark.read.csv('s3://omar-dend/trip.csv', header=True)\n",
    "st_status_df = spark.read.csv('s3://omar-dend/status.csv', header=True)\n",
    "st_city_df = spark.read.csv('s3://omar-dend/city.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save counts to ensure later that all rows are present\n",
    "station_count = st_station_df.count()\n",
    "weather_count = st_weather_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Timestamp Columns\n",
    "Each of dataframe loaded from S3 has different string represntation of date & time. In this step I add new column to represent the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_station_df = st_station_df.withColumn('datetime', F.to_timestamp(st_station_df.installation_date, 'MM/dd/yyyy'))\n",
    "\n",
    "st_weather_df = st_weather_df.withColumn('datetime', F.to_timestamp(st_weather_df.date, 'MM/dd/yyyy'))\n",
    "\n",
    "st_trip_df = st_trip_df.withColumn('datetime_start', F.to_timestamp(st_trip_df.start_date, 'MM/dd/yyyy HH:mm'))\n",
    "st_trip_df = st_trip_df.withColumn('datetime_end', F.to_timestamp(st_trip_df.end_date, 'MM/dd/yyyy HH:mm'))\n",
    "\n",
    "st_status_df = st_status_df.withColumn('datetime', F.to_timestamp(st_status_df.time, 'yyyy/MM/dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Dimensional Tables\n",
    "This is the Transformation step of the ETL process. Here relvent columns are transformed & copied to new dataframes that represent the dimensional table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Table\n",
    "Construnting `dim_time` table requires extracting the timestamps from all CSV files.\n",
    "\n",
    "Each Cell represent a different file. At the end duplicates are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = st_station_df.select('datetime')\\\n",
    "        .withColumn('second', F.second('datetime'))\\\n",
    "        .withColumn('minute', F.minute('datetime'))\\\n",
    "        .withColumn('hour', F.hour('datetime'))\\\n",
    "        .withColumn('day', F.dayofmonth('datetime'))\\\n",
    "        .withColumn('week', F.weekofyear('datetime'))\\\n",
    "        .withColumn('month', F.month('datetime'))\\\n",
    "        .withColumn('year', F.year('datetime'))\\\n",
    "        .withColumn('weekday', F.dayofweek('datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = st_weather_df.select('datetime')\\\n",
    "        .withColumn('second', F.second('datetime'))\\\n",
    "        .withColumn('minute', F.minute('datetime'))\\\n",
    "        .withColumn('hour', F.hour('datetime'))\\\n",
    "        .withColumn('day', F.dayofmonth('datetime'))\\\n",
    "        .withColumn('week', F.weekofyear('datetime'))\\\n",
    "        .withColumn('month', F.month('datetime'))\\\n",
    "        .withColumn('year', F.year('datetime'))\\\n",
    "        .withColumn('weekday', F.dayofweek('datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = st_trip_df.select(F.col('datetime_start').alias('datetime'))\\\n",
    "        .withColumn('second', F.second('datetime'))\\\n",
    "        .withColumn('minute', F.minute('datetime'))\\\n",
    "        .withColumn('hour', F.hour('datetime'))\\\n",
    "        .withColumn('day', F.dayofmonth('datetime'))\\\n",
    "        .withColumn('week', F.weekofyear('datetime'))\\\n",
    "        .withColumn('month', F.month('datetime'))\\\n",
    "        .withColumn('year', F.year('datetime'))\\\n",
    "        .withColumn('weekday', F.dayofweek('datetime'))\\\n",
    "\n",
    "time_df = st_trip_df.select(F.col('datetime_end').alias('datetime'))\\\n",
    "        .withColumn('second', F.second('datetime'))\\\n",
    "        .withColumn('minute', F.minute('datetime'))\\\n",
    "        .withColumn('hour', F.hour('datetime'))\\\n",
    "        .withColumn('day', F.dayofmonth('datetime'))\\\n",
    "        .withColumn('week', F.weekofyear('datetime'))\\\n",
    "        .withColumn('month', F.month('datetime'))\\\n",
    "        .withColumn('year', F.year('datetime'))\\\n",
    "        .withColumn('weekday', F.dayofweek('datetime'))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = st_status_df.select('datetime')\\\n",
    "        .withColumn('second', F.second('datetime'))\\\n",
    "        .withColumn('minute', F.minute('datetime'))\\\n",
    "        .withColumn('hour', F.hour('datetime'))\\\n",
    "        .withColumn('day', F.dayofmonth('datetime'))\\\n",
    "        .withColumn('week', F.weekofyear('datetime'))\\\n",
    "        .withColumn('month', F.month('datetime'))\\\n",
    "        .withColumn('year', F.year('datetime'))\\\n",
    "        .withColumn('weekday', F.dayofweek('datetime'))\\\n",
    "        .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Table\n",
    "Creating `dim_weather` table. Duplicates are dropped if any, the current dataset does not have duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = st_weather_df.select('max_temperature_f', 'mean_temperature_f', 'min_temperature_f',\n",
    "                                    'max_humidity', 'mean_humidity', 'min_humidity',\n",
    "                                    'max_wind_Speed_mph', 'mean_wind_speed_mph',\n",
    "                                    'precipitation_inches',\n",
    "                                    'events', 'zip_code', 'datetime')\\\n",
    "                                        .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Table\n",
    "Creating `dim_station` table.\n",
    "\n",
    "A new column for zip codes is added, the value is based on the name of the city. This column will make it possible to make analyses involve weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = st_station_df.select(F.col('id').alias('station_id'),\n",
    "                                    F.col('name').alias('station_name'),\n",
    "                                    'lat', 'long', 'dock_count', 'city',\n",
    "                                    F.col('datetime').alias('installation_datetime') )\n",
    "station_df = station_df.join(st_city_df, station_df.city == st_city_df.city, 'left')\\\n",
    "                            .drop('city')\\\n",
    "                            .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Table\n",
    "Creating `fact_trip` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df = st_trip_df.select(F.col('id').alias('trip_id'), 'duration', 'bike_id',\n",
    "                            'subscription_type',\n",
    "                            'start_station_id', 'end_station_id',\n",
    "                            'datetime_start', 'datetime_end')\\\n",
    "                            .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Table\n",
    "Creating `fact_status` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = st_status_df.select('station_id', 'bikes_available',\n",
    "                                'docks_available', 'datetime')\\\n",
    "                                .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all station & weather rows present\n",
    "Making sure that no station was dropped by mistake. Also making sure weather data of all the days for all zip codes are present and none was dropped by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_dim_count = station_df.count()\n",
    "weather_dim_count = weather_df.count()\n",
    "\n",
    "if station_dim_count != station_count or weather_dim_count != weather_count:\n",
    "    raise Exception('Some dimensional rows are missing')\n",
    "else:\n",
    "    print('All is good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dimensional Tables to S3\n",
    "Loading dimensional tables to S3 in parquet format. This allows data to scale (in size and number of users) infitily and letting S3 manage that.\n",
    "\n",
    "Another advantage is the AWS EMR Spark Cluster can be shutdown to save on cost while data is still accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.write.mode('overwrite')\\\n",
    "        .parquet('s3://omar-dend/dim_station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df.write.mode('overwrite')\\\n",
    "        .parquet('s3://omar-dend/fact_trip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fact_status` is partitioned by station id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df.write.mode('overwrite')\\\n",
    "        .partitionBy('station_id')\\\n",
    "        .parquet('s3://omar-dend/fact_status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dim_weather` is partitioned by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.write.mode('overwrite')\\\n",
    "        .partitionBy('zip_code')\\\n",
    "        .parquet('s3://omar-dend/dim_weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dim_time` is partitioned by year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.write.mode('overwrite')\\\n",
    "        .partitionBy('year', 'month')\\\n",
    "        .parquet('s3://omar-dend/dim_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}